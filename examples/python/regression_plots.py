# coding: utf-8

# DO NOT EDIT
# Autogenerated from the notebook regression_plots.ipynb.
# Edit the notebook and then sync the output with this file.
#
# flake8: noqa
# DO NOT EDIT

# # 回归图

from statsmodels.compat import lzip
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.formula.api import ols

# ## Duncan 的威望数据集

# ### 加载数据

# 我们可以使用一个实用函数来加载任何可用的R数据集 <a href="https://vincentarelbundock.github.io/Rdatasets/">R 数据集包</a>.

prestige = sm.datasets.get_rdataset("Duncan", "carData", cache=True).data

prestige.head()

prestige_model = ols("prestige ~ income + education", data=prestige).fit()

print(prestige_model.summary())

# ### 影响图

# 影响图显示了（外部）学生化残差与每个观察结果（通过帽子矩阵测得）的比率。
#
# 外部学生化残差是按其标准偏差缩放的残差，其中
#
# $$var(\hat{\epsilon}_i)=\hat{\sigma}^2_i(1-h_{ii})$$
#
# with
#
# $$\hat{\sigma}^2_i=\frac{1}{n - p - 1 \;\;}\sum_{j}^{n}\;\;\;\forall
# \;\;\; j \neq i$$
#
# $n$ 是观测数 $p$ 是回归数.
# $h_{ii}$ 是帽子矩阵的第 $i$-th 对角元素
#
# $$H=X(X^{\;\prime}X)^{-1}X^{\;\prime}$$
#
# 每个点的影响可以通过标准关键字参数来可视化。 选项包括 Cook 距离和 DFFITS，这是两种影响力的度量。

fig, ax = plt.subplots(figsize=(12, 8))
fig = sm.graphics.influence_plot(prestige_model, ax=ax, criterion="cooks")

# 如您所见，有一些令人担忧的观察。 承包商和记者的杠杆率均较低，但残差较大。 <br /> RR.工程师具有
# 较小的残差和较大的杠杆率。指挥官和牧师既有很高的杠杆率又有很大的残差，<br /> 因此影响很大 

# ### 偏回归图

# 由于我们正在进行多元回归，因此我们不能只看单个二元图来识别关系。 <br />相反，我们希望查看以其他自变量
# 为条件的因变量和自变量之间的关系，我们可以通过使用偏回归图（也称为添加变量图）来实现 <br />
#
# 在偏回归图中,为了辨别响应变量和第 $k$-th 变量之间的关系, 我们 <br /> 通过对响应变量与自变量（不包括 $X_k$）
# 进行回归来计算残差。我们可以用 <br /> $X_{\sim k}$ 来表示. 然后，我们通过对 $X_{\sim k}$ 回归 $X_k$ 来计算
# 残差，偏回归图是前者对后者残差的图 <br />
#
# 该图的显着点是拟合线的斜率为 $\beta_k$ ，截距为零。 此图的残差<br />与具有完整 $X$ 的原始模型的最小二乘拟合的残差相同。
# 您可以轻松地辨别<br />各个数据值对系数估计的影响。 如果 obs_labels 为 True，则这些点将使用其观察标签进行注释。 您也可以
# 看到违反基本假设，例如同方差 和<br /> 线性



fig, ax = plt.subplots(figsize=(12, 8))
fig = sm.graphics.plot_partregress(
    "prestige", "income", ["income", "education"], data=prestige, ax=ax)

fix, ax = plt.subplots(figsize=(12, 14))
fig = sm.graphics.plot_partregress(
    "prestige", "income", ["education"], data=prestige, ax=ax)

# 如您所见，偏回归图确认了指挥管、牧师和 RR.工程师对收入和声望之间的局部关系的影响。
# 这些情况大大降低了收入对声望的影响。 剔除这些情况也可以确认这一点。

subset = ~prestige.index.isin(["conductor", "RR.engineer", "minister"])
prestige_model2 = ols(
    "prestige ~ income + education", data=prestige, subset=subset).fit()
print(prestige_model2.summary())

# 要快速检查所有回归变量，可以使用 plot_partregress_grid。这些图不会标记<br />点，
# 但是您可以使用它们来识别问题，然后使用 plot_partregress 获取更多信息

fig = plt.figure(figsize=(12, 8))
fig = sm.graphics.plot_partregress_grid(prestige_model, fig=fig)

# ### Component-Component plus Residual (CCPR) Plots

# 
# CCPR 图提供了一种通过考虑其他<br />自变量的影响来判断一个回归变量对<br />响应变量的影响的方法。 偏残差图可以被定义为  <br />
# $\text{Residuals} + B_iX_i \text{ }\text{ }$ 与$ X_i $。 该组件将$ B_iX_i $与<br /> $ X_i $相加以显示拟合线的位置。 如果$ X_i $ <br />与任何其他自变量高度相关，则应格外小心。 如果是这种情况，则图中明显的方差将低估真实方差。
# The CCPR plot provides a way to judge the effect of one regressor on the
# <br />
# response variable by taking into account the effects of the other  <br
# />
# independent variables. The partial residuals plot is defined as  <br />
# $\text{Residuals} + B_iX_i \text{ }\text{ }$   versus $X_i$. The
# component adds $B_iX_i$ versus  <br />
# $X_i$ to show where the fitted line would lie. Care should be taken if
# $X_i$  <br />
# is highly correlated with any of the other independent variables. If
# this  <br />
# is the case, the variance evident in the plot will be an underestimate
# of  <br />
# the true variance.

fig, ax = plt.subplots(figsize=(12, 8))
fig = sm.graphics.plot_ccpr(prestige_model, "education", ax=ax)

# As you can see the relationship between the variation in prestige
# explained by education conditional on income seems to be linear, though
# you can see there are some observations that are exerting considerable
# influence on the relationship. We can quickly look at more than one
# variable by using plot_ccpr_grid.

fig = plt.figure(figsize=(12, 8))
fig = sm.graphics.plot_ccpr_grid(prestige_model, fig=fig)

# ### Regression Plots

# The plot_regress_exog function is a convenience function that gives a
# 2x2 plot containing the dependent variable and fitted values with
# confidence intervals vs. the independent variable chosen, the residuals of
# the model vs. the chosen independent variable, a partial regression plot,
# and a CCPR plot. This function can be used for quickly checking modeling
# assumptions with respect to a single regressor.

fig = plt.figure(figsize=(12, 8))
fig = sm.graphics.plot_regress_exog(prestige_model, "education", fig=fig)

# ### Fit Plot

# The plot_fit function plots the fitted values versus a chosen
# independent variable. It includes prediction confidence intervals and
# optionally plots the true dependent variable.

fig, ax = plt.subplots(figsize=(12, 8))
fig = sm.graphics.plot_fit(prestige_model, "education", ax=ax)

# ## Statewide Crime 2009 Dataset

# Compare the following to http://www.ats.ucla.edu/stat/stata/webbooks/reg
# /chapter4/statareg_self_assessment_answers4.htm
#
# Though the data here is not the same as in that example. You could run
# that example by uncommenting the necessary cells below.

#dta =
# pd.read_csv("http://www.stat.ufl.edu/~aa/social/csv_files/statewide-
# crime-2.csv")
#dta = dta.set_index("State", inplace=True).dropna()
#dta.rename(columns={"VR" : "crime",
#                    "MR" : "murder",
#                    "M"  : "pctmetro",
#                    "W"  : "pctwhite",
#                    "H"  : "pcths",
#                    "P"  : "poverty",
#                    "S"  : "single"
#                    }, inplace=True)
#
#crime_model = ols("murder ~ pctmetro + poverty + pcths + single",
# data=dta).fit()

dta = sm.datasets.statecrime.load_pandas().data

crime_model = ols(
    "murder ~ urban + poverty + hs_grad + single", data=dta).fit()
print(crime_model.summary())

# ### Partial Regression Plots

fig = plt.figure(figsize=(12, 8))
fig = sm.graphics.plot_partregress_grid(crime_model, fig=fig)

fig, ax = plt.subplots(figsize=(12, 8))
fig = sm.graphics.plot_partregress(
    "murder", "hs_grad", ["urban", "poverty", "single"], ax=ax, data=dta)

# ### Leverage-Resid<sup>2</sup> Plot

# Closely related to the influence_plot is the leverage-resid<sup>2</sup>
# plot.

fig, ax = plt.subplots(figsize=(8, 6))
fig = sm.graphics.plot_leverage_resid2(crime_model, ax=ax)

# ### Influence Plot

fig, ax = plt.subplots(figsize=(8, 6))
fig = sm.graphics.influence_plot(crime_model, ax=ax)

# ### Using robust regression to correct for outliers.

# Part of the problem here in recreating the Stata results is that
# M-estimators are not robust to leverage points. MM-estimators should do
# better with this examples.

from statsmodels.formula.api import rlm

rob_crime_model = rlm(
    "murder ~ urban + poverty + hs_grad + single",
    data=dta,
    M=sm.robust.norms.TukeyBiweight(3)).fit(conv="weights")
print(rob_crime_model.summary())

#rob_crime_model = rlm("murder ~ pctmetro + poverty + pcths + single",
# data=dta, M=sm.robust.norms.TukeyBiweight()).fit(conv="weights")
#print(rob_crime_model.summary())

# There is not yet an influence diagnostics method as part of RLM, but we
# can recreate them. (This depends on the status of [issue
# #888](https://github.com/statsmodels/statsmodels/issues/808))

weights = rob_crime_model.weights
idx = weights > 0
X = rob_crime_model.model.exog[idx.values]
ww = weights[idx] / weights[idx].mean()
hat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)
resid = rob_crime_model.resid
resid2 = resid**2
resid2 /= resid2.sum()
nobs = int(idx.sum())
hm = hat_matrix_diag.mean()
rm = resid2.mean()

from statsmodels.graphics import utils
fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(resid2[idx], hat_matrix_diag, 'o')
ax = utils.annotate_axes(
    range(nobs),
    labels=rob_crime_model.model.data.row_labels[idx],
    points=lzip(resid2[idx], hat_matrix_diag),
    offset_points=[(-5, 5)] * nobs,
    size="large",
    ax=ax)
ax.set_xlabel("resid2")
ax.set_ylabel("leverage")
ylim = ax.get_ylim()
ax.vlines(rm, *ylim)
xlim = ax.get_xlim()
ax.hlines(hm, *xlim)
ax.margins(0, 0)
